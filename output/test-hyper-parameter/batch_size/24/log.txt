K=8, L=1, SE_file='./data/train_data/basic/SE_data/group0/SE.txt', batch_size=24, d=8, decay_epoch=10, device='cpu', learning_rate=0.001, log_file='./output/test-hyper-parameter/batch_size/24/log.txt', max_epoch=50, model_file='./output/test-hyper-parameter/batch_size/24/model.pkl', num_his=5, num_pred=1, output_folder='./output/test-hyper-parameter/batch_size/24/', patience=100, test_ratio=0.1, time_slot=1440, traffic_file='./data/train_data/basic/train_data/mean_group0_dist3000.h5', train_ratio=0.8, val_ratio=0.1, view_batch_freq=100
main output folder./output/test-hyper-parameter/batch_size/24
loading data...
trainX: torch.Size([89, 5, 5])		 trainY: torch.Size([89, 1, 5])
valX:   torch.Size([6, 5, 5])		valY:   torch.Size([6, 1, 5])
testX:   torch.Size([7, 5, 5])		testY:   torch.Size([7, 1, 5])
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-09-25 17:41:15 | epoch: 0001/50, training time: 0.3s, inference time: 0.0s
train loss: 326559571.5955, val_loss: 273185824.0000
val loss decrease from inf to 273185824.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:15 | epoch: 0002/50, training time: 0.3s, inference time: 0.0s
train loss: 130866677.7528, val_loss: 317096864.0000
2022-09-25 17:41:15 | epoch: 0003/50, training time: 0.3s, inference time: 0.0s
train loss: 109766185.9775, val_loss: 344992992.0000
2022-09-25 17:41:16 | epoch: 0004/50, training time: 0.4s, inference time: 0.0s
train loss: 94512033.2584, val_loss: 349490016.0000
2022-09-25 17:41:16 | epoch: 0005/50, training time: 0.4s, inference time: 0.0s
train loss: 85727285.3933, val_loss: 342786208.0000
2022-09-25 17:41:16 | epoch: 0006/50, training time: 0.4s, inference time: 0.0s
train loss: 75574622.8315, val_loss: 324921024.0000
2022-09-25 17:41:17 | epoch: 0007/50, training time: 0.4s, inference time: 0.0s
train loss: 70573751.5955, val_loss: 305672768.0000
2022-09-25 17:41:17 | epoch: 0008/50, training time: 0.4s, inference time: 0.0s
train loss: 59659687.3708, val_loss: 276375680.0000
2022-09-25 17:41:18 | epoch: 0009/50, training time: 0.4s, inference time: 0.0s
train loss: 50683231.1910, val_loss: 257665616.0000
val loss decrease from 273185824.0000 to 257665616.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:18 | epoch: 0010/50, training time: 0.4s, inference time: 0.0s
train loss: 42508707.9101, val_loss: 243753408.0000
val loss decrease from 257665616.0000 to 243753408.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:19 | epoch: 0011/50, training time: 0.5s, inference time: 0.0s
train loss: 37802421.2135, val_loss: 233018576.0000
val loss decrease from 243753408.0000 to 233018576.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:19 | epoch: 0012/50, training time: 0.5s, inference time: 0.0s
train loss: 36454212.6742, val_loss: 235320272.0000
2022-09-25 17:41:20 | epoch: 0013/50, training time: 0.4s, inference time: 0.0s
train loss: 36086716.6742, val_loss: 233366528.0000
2022-09-25 17:41:20 | epoch: 0014/50, training time: 0.4s, inference time: 0.0s
train loss: 27517663.1011, val_loss: 237889856.0000
2022-09-25 17:41:21 | epoch: 0015/50, training time: 0.4s, inference time: 0.0s
train loss: 27347775.4607, val_loss: 227261488.0000
val loss decrease from 233018576.0000 to 227261488.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:21 | epoch: 0016/50, training time: 0.4s, inference time: 0.0s
train loss: 26321143.9551, val_loss: 213068720.0000
val loss decrease from 227261488.0000 to 213068720.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:21 | epoch: 0017/50, training time: 0.4s, inference time: 0.0s
train loss: 27993594.8989, val_loss: 212653264.0000
val loss decrease from 213068720.0000 to 212653264.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:22 | epoch: 0018/50, training time: 0.4s, inference time: 0.0s
train loss: 21798055.9775, val_loss: 222822192.0000
2022-09-25 17:41:22 | epoch: 0019/50, training time: 0.3s, inference time: 0.0s
train loss: 20437367.6629, val_loss: 230756432.0000
2022-09-25 17:41:23 | epoch: 0020/50, training time: 0.3s, inference time: 0.0s
train loss: 19716159.9551, val_loss: 223031280.0000
2022-09-25 17:41:23 | epoch: 0021/50, training time: 0.3s, inference time: 0.0s
train loss: 15499370.2022, val_loss: 226025776.0000
2022-09-25 17:41:23 | epoch: 0022/50, training time: 0.3s, inference time: 0.0s
train loss: 20136543.9551, val_loss: 222342288.0000
2022-09-25 17:41:24 | epoch: 0023/50, training time: 0.3s, inference time: 0.0s
train loss: 20400449.7978, val_loss: 221905808.0000
2022-09-25 17:41:24 | epoch: 0024/50, training time: 0.4s, inference time: 0.0s
train loss: 16913587.5506, val_loss: 202687792.0000
val loss decrease from 212653264.0000 to 202687792.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:24 | epoch: 0025/50, training time: 0.4s, inference time: 0.0s
train loss: 20622728.6292, val_loss: 201164208.0000
val loss decrease from 202687792.0000 to 201164208.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:25 | epoch: 0026/50, training time: 0.3s, inference time: 0.0s
train loss: 15808799.2809, val_loss: 207557328.0000
2022-09-25 17:41:25 | epoch: 0027/50, training time: 0.3s, inference time: 0.0s
train loss: 18193103.5730, val_loss: 203484528.0000
2022-09-25 17:41:25 | epoch: 0028/50, training time: 0.3s, inference time: 0.0s
train loss: 18990153.7079, val_loss: 215171472.0000
2022-09-25 17:41:26 | epoch: 0029/50, training time: 0.3s, inference time: 0.0s
train loss: 21977347.7303, val_loss: 225397808.0000
2022-09-25 17:41:26 | epoch: 0030/50, training time: 0.3s, inference time: 0.0s
train loss: 13303558.8202, val_loss: 216735888.0000
2022-09-25 17:41:27 | epoch: 0031/50, training time: 0.3s, inference time: 0.0s
train loss: 18012530.7865, val_loss: 217574096.0000
2022-09-25 17:41:27 | epoch: 0032/50, training time: 0.3s, inference time: 0.0s
train loss: 16032040.0674, val_loss: 209709520.0000
2022-09-25 17:41:27 | epoch: 0033/50, training time: 0.3s, inference time: 0.0s
train loss: 15110213.7079, val_loss: 200726384.0000
val loss decrease from 201164208.0000 to 200726384.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:28 | epoch: 0034/50, training time: 0.3s, inference time: 0.0s
train loss: 15459701.7978, val_loss: 207651152.0000
2022-09-25 17:41:28 | epoch: 0035/50, training time: 0.3s, inference time: 0.0s
train loss: 14036854.0225, val_loss: 209911376.0000
2022-09-25 17:41:28 | epoch: 0036/50, training time: 0.4s, inference time: 0.0s
train loss: 14695044.4719, val_loss: 203947472.0000
2022-09-25 17:41:29 | epoch: 0037/50, training time: 0.4s, inference time: 0.0s
train loss: 16296219.5056, val_loss: 200470608.0000
val loss decrease from 200726384.0000 to 200470608.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:29 | epoch: 0038/50, training time: 0.4s, inference time: 0.0s
train loss: 11055705.3820, val_loss: 191234112.0000
val loss decrease from 200470608.0000 to 191234112.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:30 | epoch: 0039/50, training time: 0.4s, inference time: 0.0s
train loss: 13418633.7640, val_loss: 198957136.0000
2022-09-25 17:41:30 | epoch: 0040/50, training time: 0.4s, inference time: 0.0s
train loss: 15693182.3820, val_loss: 211405008.0000
2022-09-25 17:41:30 | epoch: 0041/50, training time: 0.4s, inference time: 0.0s
train loss: 11304085.4831, val_loss: 206142976.0000
2022-09-25 17:41:31 | epoch: 0042/50, training time: 0.3s, inference time: 0.0s
train loss: 12568358.5618, val_loss: 199302592.0000
2022-09-25 17:41:31 | epoch: 0043/50, training time: 0.4s, inference time: 0.0s
train loss: 11945370.8090, val_loss: 197945536.0000
2022-09-25 17:41:32 | epoch: 0044/50, training time: 0.4s, inference time: 0.0s
train loss: 14345226.1124, val_loss: 197887216.0000
2022-09-25 17:41:32 | epoch: 0045/50, training time: 0.4s, inference time: 0.0s
train loss: 10022096.8315, val_loss: 193638736.0000
2022-09-25 17:41:32 | epoch: 0046/50, training time: 0.4s, inference time: 0.0s
train loss: 17564224.8315, val_loss: 185225520.0000
val loss decrease from 191234112.0000 to 185225520.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:33 | epoch: 0047/50, training time: 0.4s, inference time: 0.0s
train loss: 15345559.6854, val_loss: 184153088.0000
val loss decrease from 185225520.0000 to 184153088.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:33 | epoch: 0048/50, training time: 0.3s, inference time: 0.0s
train loss: 12740827.6404, val_loss: 179628480.0000
val loss decrease from 184153088.0000 to 179628480.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
2022-09-25 17:41:34 | epoch: 0049/50, training time: 0.4s, inference time: 0.0s
train loss: 14281133.2921, val_loss: 181322608.0000
2022-09-25 17:41:34 | epoch: 0050/50, training time: 0.3s, inference time: 0.0s
train loss: 14493787.3708, val_loss: 178688672.0000
val loss decrease from 179628480.0000 to 178688672.0000, saving model to ./output/test-hyper-parameter/batch_size/24/model.pkl
Training and validation are completed, and model has been stored as ./output/test-hyper-parameter/batch_size/24/model.pkl
**** testing model ****
loading model from ./output/test-hyper-parameter/batch_size/24/model.pkl
model restored!
evaluating...
testing time: 0.0s
                MAE		RMSE		MAPE
train            2607.07		3264.01		4.64%
val              10360.78		13367.45		17.00%
test             8337.73		9702.95		14.06%
performance in each prediction step
step: 01         8337.73		9702.95		14.06%
average:         8337.73		9702.95		14.06%
total time: 0.3min
