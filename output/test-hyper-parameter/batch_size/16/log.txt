K=8, L=1, SE_file='./data/train_data/basic/SE_data/group0/SE.txt', batch_size=16, d=8, decay_epoch=10, device='cpu', learning_rate=0.001, log_file='./output/test-hyper-parameter/batch_size/16/log.txt', max_epoch=50, model_file='./output/test-hyper-parameter/batch_size/16/model.pkl', num_his=5, num_pred=1, output_folder='./output/test-hyper-parameter/batch_size/16/', patience=100, test_ratio=0.1, time_slot=1440, traffic_file='./data/train_data/basic/train_data/mean_group0_dist3000.h5', train_ratio=0.8, val_ratio=0.1, view_batch_freq=100
main output folder./output/test-hyper-parameter/batch_size/16
loading data...
trainX: torch.Size([89, 5, 5])		 trainY: torch.Size([89, 1, 5])
valX:   torch.Size([6, 5, 5])		valY:   torch.Size([6, 1, 5])
testX:   torch.Size([7, 5, 5])		testY:   torch.Size([7, 1, 5])
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-09-25 17:40:45 | epoch: 0001/50, training time: 0.4s, inference time: 0.0s
train loss: 232413527.7303, val_loss: 194797264.0000
val loss decrease from inf to 194797264.0000, saving model to ./output/test-hyper-parameter/batch_size/16/model.pkl
2022-09-25 17:40:45 | epoch: 0002/50, training time: 0.4s, inference time: 0.0s
train loss: 116579571.6854, val_loss: 181854544.0000
val loss decrease from 194797264.0000 to 181854544.0000, saving model to ./output/test-hyper-parameter/batch_size/16/model.pkl
2022-09-25 17:40:46 | epoch: 0003/50, training time: 0.5s, inference time: 0.0s
train loss: 109591729.9775, val_loss: 167623120.0000
val loss decrease from 181854544.0000 to 167623120.0000, saving model to ./output/test-hyper-parameter/batch_size/16/model.pkl
2022-09-25 17:40:47 | epoch: 0004/50, training time: 0.9s, inference time: 0.0s
train loss: 96964406.7416, val_loss: 156030128.0000
val loss decrease from 167623120.0000 to 156030128.0000, saving model to ./output/test-hyper-parameter/batch_size/16/model.pkl
2022-09-25 17:40:47 | epoch: 0005/50, training time: 0.6s, inference time: 0.0s
train loss: 91787749.0337, val_loss: 154287968.0000
val loss decrease from 156030128.0000 to 154287968.0000, saving model to ./output/test-hyper-parameter/batch_size/16/model.pkl
2022-09-25 17:40:48 | epoch: 0006/50, training time: 0.5s, inference time: 0.0s
train loss: 76995741.3933, val_loss: 162699232.0000
2022-09-25 17:40:48 | epoch: 0007/50, training time: 0.5s, inference time: 0.0s
train loss: 73857414.8315, val_loss: 166724576.0000
2022-09-25 17:40:49 | epoch: 0008/50, training time: 0.6s, inference time: 0.0s
train loss: 60488164.4045, val_loss: 161699664.0000
2022-09-25 17:40:49 | epoch: 0009/50, training time: 0.4s, inference time: 0.0s
train loss: 54146755.1011, val_loss: 162319008.0000
2022-09-25 17:40:50 | epoch: 0010/50, training time: 0.5s, inference time: 0.0s
train loss: 59038234.7865, val_loss: 176982032.0000
2022-09-25 17:40:50 | epoch: 0011/50, training time: 0.5s, inference time: 0.0s
train loss: 53857773.9326, val_loss: 191310928.0000
2022-09-25 17:40:51 | epoch: 0012/50, training time: 0.5s, inference time: 0.0s
train loss: 40869463.0112, val_loss: 188597104.0000
2022-09-25 17:40:51 | epoch: 0013/50, training time: 0.5s, inference time: 0.0s
train loss: 37073880.6742, val_loss: 173725248.0000
2022-09-25 17:40:52 | epoch: 0014/50, training time: 0.5s, inference time: 0.0s
train loss: 43361348.7640, val_loss: 162006256.0000
2022-09-25 17:40:53 | epoch: 0015/50, training time: 0.5s, inference time: 0.0s
train loss: 47038744.8989, val_loss: 169504672.0000
2022-09-25 17:40:53 | epoch: 0016/50, training time: 0.4s, inference time: 0.0s
train loss: 39949014.1124, val_loss: 197136304.0000
2022-09-25 17:40:53 | epoch: 0017/50, training time: 0.5s, inference time: 0.0s
train loss: 31718195.0112, val_loss: 207519216.0000
2022-09-25 17:40:54 | epoch: 0018/50, training time: 0.4s, inference time: 0.0s
train loss: 33759140.7191, val_loss: 193566144.0000
2022-09-25 17:40:54 | epoch: 0019/50, training time: 0.4s, inference time: 0.0s
train loss: 32803076.3596, val_loss: 193210320.0000
2022-09-25 17:40:55 | epoch: 0020/50, training time: 0.5s, inference time: 0.0s
train loss: 33752687.2360, val_loss: 190726384.0000
2022-09-25 17:40:55 | epoch: 0021/50, training time: 0.5s, inference time: 0.0s
train loss: 35289148.8989, val_loss: 180901456.0000
2022-09-25 17:40:56 | epoch: 0022/50, training time: 0.4s, inference time: 0.0s
train loss: 34309055.1461, val_loss: 167688640.0000
2022-09-25 17:40:56 | epoch: 0023/50, training time: 0.4s, inference time: 0.0s
train loss: 26191337.0787, val_loss: 180362544.0000
2022-09-25 17:40:57 | epoch: 0024/50, training time: 0.5s, inference time: 0.0s
train loss: 31389169.9775, val_loss: 186782064.0000
2022-09-25 17:40:57 | epoch: 0025/50, training time: 0.5s, inference time: 0.0s
train loss: 25472890.4270, val_loss: 183014160.0000
2022-09-25 17:40:58 | epoch: 0026/50, training time: 0.5s, inference time: 0.0s
train loss: 21287130.1124, val_loss: 176200192.0000
2022-09-25 17:40:58 | epoch: 0027/50, training time: 0.4s, inference time: 0.0s
train loss: 21925867.0562, val_loss: 163167312.0000
2022-09-25 17:40:59 | epoch: 0028/50, training time: 0.4s, inference time: 0.0s
train loss: 23962511.5056, val_loss: 151266352.0000
val loss decrease from 154287968.0000 to 151266352.0000, saving model to ./output/test-hyper-parameter/batch_size/16/model.pkl
2022-09-25 17:40:59 | epoch: 0029/50, training time: 0.5s, inference time: 0.0s
train loss: 21614134.1573, val_loss: 157964352.0000
2022-09-25 17:41:00 | epoch: 0030/50, training time: 0.4s, inference time: 0.0s
train loss: 18901513.4607, val_loss: 170218560.0000
2022-09-25 17:41:00 | epoch: 0031/50, training time: 0.4s, inference time: 0.0s
train loss: 19590575.5506, val_loss: 164295232.0000
2022-09-25 17:41:01 | epoch: 0032/50, training time: 0.5s, inference time: 0.0s
train loss: 22503795.5506, val_loss: 159314496.0000
2022-09-25 17:41:01 | epoch: 0033/50, training time: 0.5s, inference time: 0.0s
train loss: 20704424.9888, val_loss: 165196144.0000
2022-09-25 17:41:02 | epoch: 0034/50, training time: 0.5s, inference time: 0.0s
train loss: 21831056.0225, val_loss: 153626432.0000
2022-09-25 17:41:02 | epoch: 0035/50, training time: 0.5s, inference time: 0.0s
train loss: 21438831.8652, val_loss: 151743264.0000
2022-09-25 17:41:03 | epoch: 0036/50, training time: 0.5s, inference time: 0.0s
train loss: 21956797.7978, val_loss: 166997056.0000
2022-09-25 17:41:03 | epoch: 0037/50, training time: 0.5s, inference time: 0.0s
train loss: 17528537.4944, val_loss: 172835520.0000
2022-09-25 17:41:04 | epoch: 0038/50, training time: 0.7s, inference time: 0.0s
train loss: 16236982.2022, val_loss: 162588720.0000
2022-09-25 17:41:05 | epoch: 0039/50, training time: 0.6s, inference time: 0.0s
train loss: 15284634.2921, val_loss: 161215968.0000
2022-09-25 17:41:05 | epoch: 0040/50, training time: 0.5s, inference time: 0.0s
train loss: 15413437.8202, val_loss: 162800416.0000
2022-09-25 17:41:06 | epoch: 0041/50, training time: 0.6s, inference time: 0.0s
train loss: 18043900.1798, val_loss: 171043504.0000
2022-09-25 17:41:06 | epoch: 0042/50, training time: 0.5s, inference time: 0.0s
train loss: 21364437.0337, val_loss: 175412064.0000
2022-09-25 17:41:07 | epoch: 0043/50, training time: 0.4s, inference time: 0.0s
train loss: 15498192.2022, val_loss: 163667312.0000
2022-09-25 17:41:07 | epoch: 0044/50, training time: 0.4s, inference time: 0.0s
train loss: 24587962.7865, val_loss: 169698992.0000
2022-09-25 17:41:08 | epoch: 0045/50, training time: 0.4s, inference time: 0.0s
train loss: 13699047.1573, val_loss: 164188016.0000
2022-09-25 17:41:08 | epoch: 0046/50, training time: 0.5s, inference time: 0.0s
train loss: 18617812.1573, val_loss: 160250400.0000
2022-09-25 17:41:09 | epoch: 0047/50, training time: 0.4s, inference time: 0.0s
train loss: 17111172.3146, val_loss: 166109792.0000
2022-09-25 17:41:09 | epoch: 0048/50, training time: 0.4s, inference time: 0.0s
train loss: 19116436.8989, val_loss: 172249136.0000
2022-09-25 17:41:09 | epoch: 0049/50, training time: 0.4s, inference time: 0.0s
train loss: 13404252.6180, val_loss: 175793696.0000
2022-09-25 17:41:10 | epoch: 0050/50, training time: 0.4s, inference time: 0.0s
train loss: 21414948.5843, val_loss: 176646320.0000
Training and validation are completed, and model has been stored as ./output/test-hyper-parameter/batch_size/16/model.pkl
**** testing model ****
loading model from ./output/test-hyper-parameter/batch_size/16/model.pkl
model restored!
evaluating...
testing time: 0.0s
                MAE		RMSE		MAPE
train            2930.58		3850.28		5.50%
val              10777.83		13290.84		18.59%
test             8131.80		9724.15		13.83%
performance in each prediction step
step: 01         8131.80		9724.15		13.83%
average:         8131.80		9724.15		13.83%
total time: 0.4min
