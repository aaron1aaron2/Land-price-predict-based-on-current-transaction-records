K=8, L=1, SE_file='./data/train_data/basic/SE_data/group0/SE.txt', batch_size=16, d=8, decay_epoch=10, device='cpu', learning_rate=0.0001, log_file='./output/test-hyper-parameter(price)/learning_rate/0.0001/log.txt', max_epoch=50, model_file='./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl', num_his=3, num_pred=1, output_folder='./output/test-hyper-parameter(price)/learning_rate/0.0001', patience=100, test_ratio=0.1, time_slot=1440, traffic_file='./data/train_data/basic/train_data/mean_group0_dist3000.h5', train_ratio=0.8, val_ratio=0.1, view_batch_freq=100
main output folder./output/test-hyper-parameter(price)/learning_rate/0.0001
loading data...
trainX: torch.Size([91, 3, 5])		 trainY: torch.Size([91, 1, 5])
valX:   torch.Size([8, 3, 5])		valY:   torch.Size([8, 1, 5])
testX:   torch.Size([9, 3, 5])		testY:   torch.Size([9, 1, 5])
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-09-26 19:48:48 | epoch: 0001/50, training time: 0.5s, inference time: 0.0s
train loss: 399382249.4945, val_loss: 199884064.0000
val loss decrease from inf to 199884064.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:48:49 | epoch: 0002/50, training time: 0.6s, inference time: 0.0s
train loss: 312780096.0000, val_loss: 214894464.0000
2022-09-26 19:48:50 | epoch: 0003/50, training time: 0.7s, inference time: 0.0s
train loss: 248397084.6593, val_loss: 215699632.0000
2022-09-26 19:48:50 | epoch: 0004/50, training time: 0.6s, inference time: 0.0s
train loss: 214564564.0440, val_loss: 229020848.0000
2022-09-26 19:48:51 | epoch: 0005/50, training time: 0.6s, inference time: 0.0s
train loss: 180974051.3407, val_loss: 232329040.0000
2022-09-26 19:48:51 | epoch: 0006/50, training time: 0.5s, inference time: 0.0s
train loss: 173838890.5495, val_loss: 244796832.0000
2022-09-26 19:48:52 | epoch: 0007/50, training time: 0.7s, inference time: 0.0s
train loss: 148137351.9121, val_loss: 259828944.0000
2022-09-26 19:48:53 | epoch: 0008/50, training time: 0.8s, inference time: 0.0s
train loss: 134609759.4725, val_loss: 243413680.0000
2022-09-26 19:48:54 | epoch: 0009/50, training time: 0.8s, inference time: 0.0s
train loss: 120859554.4615, val_loss: 242809008.0000
2022-09-26 19:48:55 | epoch: 0010/50, training time: 0.8s, inference time: 0.0s
train loss: 118127700.6593, val_loss: 225105024.0000
2022-09-26 19:48:55 | epoch: 0011/50, training time: 0.8s, inference time: 0.0s
train loss: 113662278.4176, val_loss: 227173840.0000
2022-09-26 19:48:56 | epoch: 0012/50, training time: 0.7s, inference time: 0.0s
train loss: 116980944.8791, val_loss: 231788000.0000
2022-09-26 19:48:57 | epoch: 0013/50, training time: 0.8s, inference time: 0.0s
train loss: 104284051.8681, val_loss: 224839168.0000
2022-09-26 19:48:58 | epoch: 0014/50, training time: 0.7s, inference time: 0.0s
train loss: 104483905.0549, val_loss: 199210336.0000
val loss decrease from 199884064.0000 to 199210336.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:48:58 | epoch: 0015/50, training time: 0.7s, inference time: 0.0s
train loss: 102415687.2088, val_loss: 194194128.0000
val loss decrease from 199210336.0000 to 194194128.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:48:59 | epoch: 0016/50, training time: 0.7s, inference time: 0.0s
train loss: 100868028.3077, val_loss: 185847808.0000
val loss decrease from 194194128.0000 to 185847808.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:49:00 | epoch: 0017/50, training time: 0.7s, inference time: 0.0s
train loss: 96002976.2637, val_loss: 176527840.0000
val loss decrease from 185847808.0000 to 176527840.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:49:01 | epoch: 0018/50, training time: 0.8s, inference time: 0.0s
train loss: 92426106.9011, val_loss: 167242176.0000
val loss decrease from 176527840.0000 to 167242176.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:49:02 | epoch: 0019/50, training time: 0.8s, inference time: 0.0s
train loss: 86540082.9011, val_loss: 165963232.0000
val loss decrease from 167242176.0000 to 165963232.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:49:02 | epoch: 0020/50, training time: 0.7s, inference time: 0.0s
train loss: 93932514.9011, val_loss: 161601184.0000
val loss decrease from 165963232.0000 to 161601184.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:49:03 | epoch: 0021/50, training time: 0.8s, inference time: 0.0s
train loss: 89009985.3187, val_loss: 160511520.0000
val loss decrease from 161601184.0000 to 160511520.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:49:04 | epoch: 0022/50, training time: 0.8s, inference time: 0.0s
train loss: 78632257.9341, val_loss: 154624080.0000
val loss decrease from 160511520.0000 to 154624080.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:49:05 | epoch: 0023/50, training time: 0.8s, inference time: 0.0s
train loss: 73565112.8791, val_loss: 153203152.0000
val loss decrease from 154624080.0000 to 153203152.0000, saving model to ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
2022-09-26 19:49:06 | epoch: 0024/50, training time: 0.7s, inference time: 0.0s
train loss: 82934110.8571, val_loss: 159100192.0000
2022-09-26 19:49:07 | epoch: 0025/50, training time: 0.8s, inference time: 0.0s
train loss: 78204496.4835, val_loss: 158753872.0000
2022-09-26 19:49:07 | epoch: 0026/50, training time: 0.7s, inference time: 0.0s
train loss: 75319335.0330, val_loss: 156638944.0000
2022-09-26 19:49:08 | epoch: 0027/50, training time: 0.8s, inference time: 0.0s
train loss: 81601379.0769, val_loss: 165056032.0000
2022-09-26 19:49:09 | epoch: 0028/50, training time: 0.7s, inference time: 0.0s
train loss: 79026835.7802, val_loss: 163082816.0000
2022-09-26 19:49:10 | epoch: 0029/50, training time: 0.7s, inference time: 0.0s
train loss: 66773752.8791, val_loss: 159716016.0000
2022-09-26 19:49:10 | epoch: 0030/50, training time: 0.7s, inference time: 0.0s
train loss: 68408863.8242, val_loss: 157036960.0000
2022-09-26 19:49:11 | epoch: 0031/50, training time: 0.8s, inference time: 0.0s
train loss: 78800356.9231, val_loss: 159366336.0000
2022-09-26 19:49:12 | epoch: 0032/50, training time: 0.7s, inference time: 0.0s
train loss: 73141702.0659, val_loss: 166840608.0000
2022-09-26 19:49:13 | epoch: 0033/50, training time: 0.8s, inference time: 0.0s
train loss: 72438223.2088, val_loss: 165735136.0000
2022-09-26 19:49:14 | epoch: 0034/50, training time: 0.7s, inference time: 0.0s
train loss: 62382326.3297, val_loss: 170534240.0000
2022-09-26 19:49:14 | epoch: 0035/50, training time: 0.7s, inference time: 0.0s
train loss: 62910737.6264, val_loss: 161519184.0000
2022-09-26 19:49:15 | epoch: 0036/50, training time: 0.8s, inference time: 0.0s
train loss: 69086388.3516, val_loss: 153997264.0000
2022-09-26 19:49:16 | epoch: 0037/50, training time: 0.7s, inference time: 0.0s
train loss: 65806729.4945, val_loss: 153730624.0000
2022-09-26 19:49:17 | epoch: 0038/50, training time: 0.8s, inference time: 0.0s
train loss: 67814920.0879, val_loss: 154074592.0000
2022-09-26 19:49:18 | epoch: 0039/50, training time: 0.8s, inference time: 0.0s
train loss: 54623518.9890, val_loss: 161241504.0000
2022-09-26 19:49:18 | epoch: 0040/50, training time: 0.8s, inference time: 0.0s
train loss: 63489099.6923, val_loss: 164076784.0000
2022-09-26 19:49:19 | epoch: 0041/50, training time: 0.8s, inference time: 0.0s
train loss: 58303509.3626, val_loss: 168700032.0000
2022-09-26 19:49:20 | epoch: 0042/50, training time: 0.7s, inference time: 0.0s
train loss: 55685987.7363, val_loss: 173237728.0000
2022-09-26 19:49:21 | epoch: 0043/50, training time: 0.8s, inference time: 0.0s
train loss: 61416937.9341, val_loss: 173705136.0000
2022-09-26 19:49:21 | epoch: 0044/50, training time: 0.7s, inference time: 0.0s
train loss: 57301098.3297, val_loss: 171853424.0000
2022-09-26 19:49:22 | epoch: 0045/50, training time: 0.8s, inference time: 0.0s
train loss: 61376512.1758, val_loss: 176294912.0000
2022-09-26 19:49:23 | epoch: 0046/50, training time: 0.8s, inference time: 0.0s
train loss: 54304917.8901, val_loss: 173374496.0000
2022-09-26 19:49:24 | epoch: 0047/50, training time: 0.7s, inference time: 0.0s
train loss: 50639574.8571, val_loss: 170081504.0000
2022-09-26 19:49:25 | epoch: 0048/50, training time: 0.7s, inference time: 0.0s
train loss: 54792853.1868, val_loss: 171667760.0000
2022-09-26 19:49:25 | epoch: 0049/50, training time: 0.7s, inference time: 0.0s
train loss: 55003164.6593, val_loss: 167411568.0000
2022-09-26 19:49:26 | epoch: 0050/50, training time: 0.7s, inference time: 0.0s
train loss: 59737375.2088, val_loss: 175501088.0000
Training and validation are completed, and model has been stored as ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
**** testing model ****
loading model from ./output/test-hyper-parameter(price)/learning_rate/0.0001/model.pkl
model restored!
evaluating...
testing time: 0.0s
                MAE		RMSE		MAPE
train            5147.29		6631.91		9.64%
val              9690.74		13247.68		17.73%
test             9630.82		11753.53		16.28%
performance in each prediction step
step: 01         9630.82		11753.53		16.28%
average:         9630.82		11753.53		16.28%
total time: 0.7min
