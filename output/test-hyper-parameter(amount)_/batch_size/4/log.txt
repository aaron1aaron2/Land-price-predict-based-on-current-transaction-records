K=8, L=1, SE_file='./data/train_data/transaction_amount/SE_data/group0/SE.txt', batch_size=4, d=8, decay_epoch=10, device='cpu', learning_rate=0.001, log_file='./output/test-hyper-parameter(amount)_/batch_size/4/log.txt', max_epoch=50, model_file='./output/test-hyper-parameter(amount)_/batch_size/4/model.pkl', num_his=3, num_pred=1, output_folder='./output/test-hyper-parameter(amount)_/batch_size/4/', patience=100, test_ratio=0.1, time_slot=1440, traffic_file='./data/train_data/transaction_amount/train_data/count_group0_dist3000.h5', train_ratio=0.8, val_ratio=0.1, view_batch_freq=100
main output folder./output/test-hyper-parameter(amount)_/batch_size/4
loading data...
trainX: torch.Size([91, 3, 5])		 trainY: torch.Size([91, 1, 5])
valX:   torch.Size([8, 3, 5])		valY:   torch.Size([8, 1, 5])
testX:   torch.Size([9, 3, 5])		testY:   torch.Size([9, 1, 5])
data loaded!
compiling model...
trainable parameters: 209,923
**** training model ****
2022-09-26 19:50:23 | epoch: 0001/50, training time: 2.1s, inference time: 0.0s
train loss: 947.9149, val_loss: 482.6691
val loss decrease from inf to 482.6691, saving model to ./output/test-hyper-parameter(amount)_/batch_size/4/model.pkl
2022-09-26 19:50:25 | epoch: 0002/50, training time: 1.8s, inference time: 0.0s
train loss: 715.2786, val_loss: 415.2885
val loss decrease from 482.6691 to 415.2885, saving model to ./output/test-hyper-parameter(amount)_/batch_size/4/model.pkl
2022-09-26 19:50:26 | epoch: 0003/50, training time: 1.7s, inference time: 0.0s
train loss: 695.4254, val_loss: 481.1797
2022-09-26 19:50:28 | epoch: 0004/50, training time: 1.3s, inference time: 0.0s
train loss: 650.2084, val_loss: 747.0181
2022-09-26 19:50:29 | epoch: 0005/50, training time: 1.3s, inference time: 0.0s
train loss: 645.4810, val_loss: 620.0209
2022-09-26 19:50:30 | epoch: 0006/50, training time: 1.2s, inference time: 0.0s
train loss: 465.5259, val_loss: 661.5909
2022-09-26 19:50:31 | epoch: 0007/50, training time: 1.2s, inference time: 0.0s
train loss: 572.4508, val_loss: 787.2390
2022-09-26 19:50:33 | epoch: 0008/50, training time: 1.2s, inference time: 0.0s
train loss: 658.6228, val_loss: 870.8876
2022-09-26 19:50:34 | epoch: 0009/50, training time: 1.2s, inference time: 0.0s
train loss: 613.6663, val_loss: 916.6848
2022-09-26 19:50:35 | epoch: 0010/50, training time: 1.2s, inference time: 0.0s
train loss: 535.1040, val_loss: 1021.7667
2022-09-26 19:50:36 | epoch: 0011/50, training time: 1.2s, inference time: 0.0s
train loss: 566.1344, val_loss: 1059.3024
2022-09-26 19:50:38 | epoch: 0012/50, training time: 1.2s, inference time: 0.0s
train loss: 440.6223, val_loss: 761.7323
2022-09-26 19:50:39 | epoch: 0013/50, training time: 1.2s, inference time: 0.0s
train loss: 623.7007, val_loss: 740.7376
2022-09-26 19:50:40 | epoch: 0014/50, training time: 1.2s, inference time: 0.0s
train loss: 455.7118, val_loss: 946.9701
2022-09-26 19:50:41 | epoch: 0015/50, training time: 1.2s, inference time: 0.0s
train loss: 454.2773, val_loss: 684.8199
2022-09-26 19:50:43 | epoch: 0016/50, training time: 1.2s, inference time: 0.0s
train loss: 461.9082, val_loss: 572.9780
2022-09-26 19:50:44 | epoch: 0017/50, training time: 1.2s, inference time: 0.0s
train loss: 400.8296, val_loss: 576.8511
2022-09-26 19:50:45 | epoch: 0018/50, training time: 1.2s, inference time: 0.0s
train loss: 521.5920, val_loss: 630.5095
2022-09-26 19:50:46 | epoch: 0019/50, training time: 1.2s, inference time: 0.0s
train loss: 422.2028, val_loss: 641.0894
2022-09-26 19:50:48 | epoch: 0020/50, training time: 1.2s, inference time: 0.0s
train loss: 505.3368, val_loss: 645.4596
2022-09-26 19:50:49 | epoch: 0021/50, training time: 1.2s, inference time: 0.0s
train loss: 553.7910, val_loss: 578.8311
2022-09-26 19:50:50 | epoch: 0022/50, training time: 1.2s, inference time: 0.0s
train loss: 456.9129, val_loss: 564.1207
2022-09-26 19:50:51 | epoch: 0023/50, training time: 1.2s, inference time: 0.0s
train loss: 541.1341, val_loss: 694.5348
2022-09-26 19:50:52 | epoch: 0024/50, training time: 1.2s, inference time: 0.0s
train loss: 436.4664, val_loss: 597.5051
2022-09-26 19:50:54 | epoch: 0025/50, training time: 1.2s, inference time: 0.0s
train loss: 470.1331, val_loss: 497.6082
2022-09-26 19:50:55 | epoch: 0026/50, training time: 1.2s, inference time: 0.0s
train loss: 489.4791, val_loss: 673.3231
2022-09-26 19:50:56 | epoch: 0027/50, training time: 1.2s, inference time: 0.0s
train loss: 480.0796, val_loss: 795.3405
2022-09-26 19:50:57 | epoch: 0028/50, training time: 1.2s, inference time: 0.0s
train loss: 470.6585, val_loss: 618.9874
2022-09-26 19:50:59 | epoch: 0029/50, training time: 1.2s, inference time: 0.0s
train loss: 431.1914, val_loss: 774.5030
2022-09-26 19:51:00 | epoch: 0030/50, training time: 1.2s, inference time: 0.0s
train loss: 459.1276, val_loss: 688.8353
2022-09-26 19:51:01 | epoch: 0031/50, training time: 1.2s, inference time: 0.0s
train loss: 432.1185, val_loss: 631.0009
2022-09-26 19:51:02 | epoch: 0032/50, training time: 1.2s, inference time: 0.0s
train loss: 508.9737, val_loss: 596.3549
2022-09-26 19:51:03 | epoch: 0033/50, training time: 1.2s, inference time: 0.0s
train loss: 470.2284, val_loss: 652.8322
2022-09-26 19:51:05 | epoch: 0034/50, training time: 1.2s, inference time: 0.0s
train loss: 617.8891, val_loss: 915.0496
2022-09-26 19:51:06 | epoch: 0035/50, training time: 1.2s, inference time: 0.0s
train loss: 508.7662, val_loss: 761.2236
2022-09-26 19:51:07 | epoch: 0036/50, training time: 1.2s, inference time: 0.0s
train loss: 513.4126, val_loss: 537.5548
2022-09-26 19:51:08 | epoch: 0037/50, training time: 1.2s, inference time: 0.0s
train loss: 430.2994, val_loss: 576.1908
2022-09-26 19:51:10 | epoch: 0038/50, training time: 1.1s, inference time: 0.0s
train loss: 362.7431, val_loss: 667.5306
2022-09-26 19:51:11 | epoch: 0039/50, training time: 1.2s, inference time: 0.0s
train loss: 491.5503, val_loss: 626.8087
2022-09-26 19:51:12 | epoch: 0040/50, training time: 1.1s, inference time: 0.0s
train loss: 443.9983, val_loss: 555.5472
2022-09-26 19:51:13 | epoch: 0041/50, training time: 1.2s, inference time: 0.0s
train loss: 414.2553, val_loss: 702.5189
2022-09-26 19:51:14 | epoch: 0042/50, training time: 1.2s, inference time: 0.0s
train loss: 455.0025, val_loss: 655.6664
2022-09-26 19:51:16 | epoch: 0043/50, training time: 1.3s, inference time: 0.0s
train loss: 439.3363, val_loss: 623.4773
2022-09-26 19:51:17 | epoch: 0044/50, training time: 1.4s, inference time: 0.0s
train loss: 464.0889, val_loss: 492.6790
2022-09-26 19:51:18 | epoch: 0045/50, training time: 1.2s, inference time: 0.0s
train loss: 477.3177, val_loss: 668.2454
2022-09-26 19:51:20 | epoch: 0046/50, training time: 1.2s, inference time: 0.0s
train loss: 431.5454, val_loss: 472.9006
2022-09-26 19:51:21 | epoch: 0047/50, training time: 1.2s, inference time: 0.0s
train loss: 423.6234, val_loss: 724.7954
2022-09-26 19:51:22 | epoch: 0048/50, training time: 1.2s, inference time: 0.0s
train loss: 505.7016, val_loss: 556.4062
2022-09-26 19:51:23 | epoch: 0049/50, training time: 1.2s, inference time: 0.0s
train loss: 509.3352, val_loss: 674.6058
2022-09-26 19:51:25 | epoch: 0050/50, training time: 1.2s, inference time: 0.0s
train loss: 479.1277, val_loss: 604.5630
Training and validation are completed, and model has been stored as ./output/test-hyper-parameter(amount)_/batch_size/4/model.pkl
**** testing model ****
loading model from ./output/test-hyper-parameter(amount)_/batch_size/4/model.pkl
model restored!
evaluating...
testing time: 0.0s
                MAE		RMSE		MAPE
train            12.37		17.68		28.96%
val              20.48		24.59		51.70%
test             11.26		14.50		38.59%
performance in each prediction step
step: 01         11.26		14.50		38.59%
average:         11.26		14.50		38.59%
total time: 1.1min
