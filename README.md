# PropGman: Regional-index-predict-based-on-transaction-records

![](img/æµç¨‹åœ–.png)
## Taget ğŸ¯ 
æœ¬ç ”ç©¶å¸Œæœ›å»ºç«‹ä¸€å¥—é€šç”¨æµç¨‹ï¼Œé€éå¯¦åƒ¹ç™»éŒ„è³‡æ–™ï¼Œå°åœŸåœ°åƒ¹å€¼é€²è¡Œé æ¸¬ï¼Œå¹«åŠ©ä¸è«–æ˜¯å…¬éƒ¨é–€æˆ–æ˜¯ç›¸é—œäººå£«åœ¨å°æ–¼åœŸåœ°è³‡ç”¢æ´»åŒ–çš„è©•ä¼°èˆ‡æ±ºç­–ä¸­ï¼Œä¸€äº›éå»éœ€ä»¥äººå·¥ç¶“é©—ã€èªçŸ¥åˆ¤æ–·çš„å› ç´ èˆ‡æŒ‡æ¨™ï¼Œé€šéä¸€ç¨®æ›´ç‚ºå®¢è§€æœ‰ä¾æ“šçš„æ–¹æ³•é€²è¡Œæ›´ç²¾æº–çš„é æ¸¬ã€‚
å½±éŸ¿åœŸåœ°çš„åƒ¹å€¼æœ‰è¨±å¤šä¸åŒçš„æŒ‡æ¨™ï¼Œæœ¬ç ”ç©¶é¸ä»¥ã€Œä¸€æ®µæ™‚é–“èˆ‡å€åŸŸç¯„åœå…§çš„åœŸåœ°äº¤æ˜“ä¹‹å¹³å‡æ¯åªåœŸåœ°äº¤æ˜“åƒ¹å€¼ã€ç‚ºç¯„ä¾‹ç›®æ¨™é€²è¡Œæ·±å…¥è¨è«–èˆ‡ç ”ç©¶ï¼Œä¸¦æœŸè¨±æ–¼æœªä¾†å¯ä»¥å°‡æ­¤æµç¨‹èˆ‡æ–¹æ³•æ“´å±•è‡³å…¶ä»–æŒ‡æ¨™ä¸Šï¼Œé€²è¡Œæ›´å…¨é¢ã€å®Œå–„çš„æ¢è¨èˆ‡é‹ç”¨ï¼Œé”åˆ°çœŸæ­£çš„è³‡ç”¢æ´»åŒ–èˆ‡æ•ˆç›Šæœ€å¤§åŒ–ã€‚

## Result 
### `æ¯æœˆå–®ä½åœŸåœ°åƒ¹`
æ¯å€‹æœˆåœ¨ç›®æ¨™åœŸåœ°ä¸­å¿ƒ 3000 å…¬å°ºå…§çš„åœŸåœ°äº¤æ˜“(æ™‚åƒ¹ç™»å…¥)çš„ã€Œå–®åƒ¹å…ƒå¹³æ–¹å…¬å°ºã€å¹³å‡å€¼ã€‚
| Group | MAE | RMSE | MAPE |
| --- | --- | --- | --- |
| 0 | 8559 | 11221 | 0.1200 |
| 1 | 13177 | 15033 | 0.1714 |
| 2 | 4725 | 6197 | 0.1551 |
| 3 | 12471 | 15810 | 0.1817 |
| 4 | 36140 | 43242 | 0.5367 |
| 5 | 36388 | 37937 | 0.6182 |
| 6 | 18018 | 20783 | 0.3152 |
| 7 | 4718 | 6186 | 0.2148 |
| 8 | 3126 | 4391 | 0.1766 |
| 9 | 27635 | 32195 | 0.4216 |
| 10 | 8805 | 10027 | 0.3290 |
|  |  | avg. | 0.29 |

### `æ¯æœˆäº¤æ˜“é‡`
æ¯å€‹æœˆåœ¨ç›®æ¨™åœŸåœ°ä¸­å¿ƒ 3000 å…¬å°ºå…§çš„åœŸåœ°äº¤æ˜“(æ™‚åƒ¹ç™»å…¥)çš„æ•¸é‡ã€‚
| Group | MAE | RMSE | MAPE |
| --- | --- | --- | --- |
| 0 | 16.3 | 22.4 | 0.4673 |
| 1 | 16.3 | 20.5 | 0.4492 |
| 2 | 8.6 | 10.5 | 0.4751 |
| 3 | 10.3 | 12.3 | 0.3869 |
| 4 | 8.3 | 10.2 | 0.7115 |
| 5 | 7.5 | 8.9 | 1.1352 |
| 6 | 4.3 | 4.4 | 0.7005 |
| 7 | 7.5 | 9.5 | 0.4964 |
| 8 | 7.5 | 8.6 | 0.4324 |
| 9 | 10.8 | 13.2 | 1.8828 |
| 10 | 7.9 | 10.5 | 0.6284 |
|  |  | avg. | 0.706 |

## `Demo`
### input
| year | month | group_center | refer_point1 | refer_point2 | refer_point3 | refer_point4 | group |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 2017 | 11 | 63555 | 57228 | 73875 | 63721 | 37824 | 0 |
| 2017 | 12 | 47273 | 40341 | 49434 | 58276 | 32960 | 0 |
| 2018 | 1 | 45804 | 60585 | 44698 | 46390 | 34342 | 0 |


### our predict
```
python demo.py
```
![](img/demo_res.png)

ps. ç­”æ¡ˆç‚º 72275ï¼Œé›–ç„¶å·®äº†1è¬å·¦å³ï¼Œä½†æ˜¯ä»¥è¶¨å‹¢ä¾†çœ‹ï¼Œè¡¨ç¾ä¸éŒ¯ã€‚

# Reproduce
## ğŸ“ Folder schema 
```
- PropGman
    |-- method: æˆ‘å€‘çš„æ–¹æ³•
    |-- model: åŸå§‹ gman code

- EDA_and_preprocess
    |-- code: è³‡æ–™çˆ¬èŸ²ã€å‰è™•ç†ã€æ¢å‹˜éç¨‹
        |--data_procces
        |--method_procces
        |--supplementary: ééç¨‹ä¸­çš„å…¶ä»–åƒè€ƒç¨‹å¼ç¢¼

- configs
- data 
    |-- input: æ•´ç†éçš„è³‡æ–™ï¼ŒåŒ…å«ç›®æ¨™åœŸåœ° & æ™‚åƒ¹ç™»å…¥
    |-- train_data: è¨“ç·´ç”¨è³‡æ–™

- output
- scripts: shell or batch è…³æœ¬ï¼ŒåŒ…å«æ‰¹æ¬¡è·‘å¯¦é©—ã€è¨“ç·´ç¯„ä¾‹
- doc
- img

- requirements.txt: python ä¾è³´å¥—ä»¶
- data_helper.py: å°‡è¼¸å…¥è³‡æ–™(data/input)ä¾é€ åƒæ•¸è¨­å®šæª”(configs)è½‰æ›æˆè¨“ç·´è³‡æ–™(data/train_data)
- train.py: ä¸»è¦è¨“ç·´ç¨‹å¼ç¢¼
```
## ğŸ–¥ï¸ Environment settings 
### `code`
```shell
git clone https://github.com/aaron1aaron2/PropGman__Regional-index-predict-based-on-transaction-records.git
```
### `pytorch`
æœ¬å°ˆæ¡ˆæ˜¯åœ¨ window 11ã€cuda(11.6)ã€pytorch(1.12.1)æ¸¬è©¦ã€‚
å¦‚ä½¿ç”¨ä¸åŒç’°å¢ƒè«‹åˆ° [pytorch å®˜ç¶²](https://pytorch.org/) é¸æ“‡å°æ‡‰ç‰ˆçš„æŒ‡ä»¤ã€‚
```shell
pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
```

### `other packages`
```shell
pip3 install -r requirements.txt
```
## ğŸ™‹ Quick start 
### `Step1: è³‡æ–™æº–å‚™`
ä¸»è¦è³‡æ–™ä½æ–¼ *data/input* åº•ä¸‹ã€‚
- target.csv: ç›®æ¨™åœŸåœ°è³‡æ–™ + åœŸåœ°ç¶“ç·¯åº¦
- transaction.csv: æ™‚åƒ¹ç™»å…¥è³‡æ–™ + åœŸåœ°ç¶“ç·¯åº¦

p.s. åœŸåœ°ç¶“ç·¯çˆ¬å–èˆ‡è³‡æ–™å‰è™•ç†æµç¨‹è«‹åƒè€ƒ *EDA_and_preprocess*

### `Step2: ç”¢ç”Ÿè¨“ç·´è³‡æ–™`

```shell
python data_helper.py --config_path configs/Basic.yaml
```
p.s. å¯ä»¥åˆ° *configs* è³‡æ–™å¤¾åº•ä¸‹è¤‡è£½æ¨¡æ¿ï¼Œä¸¦ä¾è‡ªå·±çš„éœ€æ±‚å®¢è£½åŒ–è‡ªå·±çš„ config æª”ã€‚

### `Step3: è¨“ç·´æ¨¡å‹`
```shell
scripts\train_basic.bat
```
or
```shell
source scripts/train_basic.sh
```
p.s. *scripts* åº•ä¸‹æœ‰æ‰¹æ¬¡è·‘å¯¦é©—æ¨¡æ¿ï¼Œå¯ä»¥åƒè€ƒã€‚
# Other information
## Our team
|å§“å|å­¸æ ¡|ç³»ç´š|github|
|-|-|-|-|
|ä½•å½¥å—|åœ‹ç«‹æ”¿æ²»å¤§å­¸(NCCU)|è³‡ç§‘ç¢©äºŒ(æ™ºæ…§è¨ˆç®—çµ„)|https://github.com/aaron1aaron2|
|èŠå´´å®‡|åœ‹ç«‹æ”¿æ²»å¤§å­¸(NCCU)|è³‡ç§‘ç¢©äºŒ(ä¸€èˆ¬çµ„)||https://github.com/C-WeiYu|
|å‘¨å€¢å› |åœ‹ç«‹å°ç£ç§‘æŠ€å¤§å­¸(NTUST)|è³‡ç§‘ç¢©äºŒ|

## Code Source (GMAN)
[![](https://github-readme-stats.vercel.app/api/pin/?username=VincLee8188&repo=GMAN-PyTorch)](https://github.com/VincLee8188/GMAN-PyTorch)

## Citation

This version of implementation is only for learning purpose. For research, please refer to  and  cite from the following paper:
```
@inproceedings{ GMAN-AAAI2020,
  author = "Chuanpan Zheng and Xiaoliang Fan and Cheng Wang and Jianzhong Qi"
  title = "GMAN: A Graph Multi-Attention Network for Traffic Prediction",
  booktitle = "AAAI",
  pages = "1234--1241",
  year = "2020"
}
```
